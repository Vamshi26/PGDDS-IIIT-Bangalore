{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Parking Case Study \n",
    "\n",
    "The purpose of this case study is to conduct an exploratory data analysis that will help us understand the classic combination of a huge number of cars and cramped geography leading to the biggest problem of New York, that is parking.\n",
    "\n",
    "In an attempt to scientifically analyse this phenomenon, the NYC Police Department has collected data for parking tickets.\n",
    "\n",
    "We will perform some exploratory analysis on a part of this data. Spark will allow us to analyse the full files at high speeds as opposed to taking a series of random samples that will approximate the population. For the scope of this analysis, we will analyse the parking tickets over the year 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SparkSession is used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and \n",
    "# read parquet files.\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"NYC Taxi Parking\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons Number: bigint, Plate ID: string, Registration State: string, Issue Date: timestamp, Violation Code: int, Vehicle Body Type: string, Vehicle Make: string, Violation Precinct: int, Issuer Precinct: int, Violation Time: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data file\n",
    "nyc_taxi = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "                             .load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")\n",
    "nyc_taxi.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 records\n",
    "nyc_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows in the dataset\n",
    "nyc_taxi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of columns in the dataset\n",
    "len(nyc_taxi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summons_Number',\n",
       " 'Plate_ID',\n",
       " 'Registration_State',\n",
       " 'Issue_Date',\n",
       " 'Violation_Code',\n",
       " 'Vehicle_Body_Type',\n",
       " 'Vehicle_Make',\n",
       " 'Violation_Precinct',\n",
       " 'Issuer_Precinct',\n",
       " 'Violation_Time']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since Column Names of the dataframes have spaces in between consecutive words it may lead to typos and coding issues. \n",
    "# Therefore, we will eliminate all white spaces and seperate consecutive words with \"_\"\n",
    "nyc_taxi = nyc_taxi.toDF(*[col.replace(' ','_') for col in nyc_taxi.columns])\n",
    "nyc_taxi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons_Number|Plate_ID|Registration_State|         Issue_Date|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top five records\n",
    "nyc_taxi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: long (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Issue_Date: timestamp (nullable = true)\n",
      " |-- Violation_Code: integer (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: integer (nullable = true)\n",
      " |-- Issuer_Precinct: integer (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema of the dataframe\n",
    "nyc_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register DataFrame as temp table\n",
    "nyc_taxi.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Find the total number of tickets for the year.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|10803028|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(1) FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|issue_year|num_records|\n",
      "+----------+-----------+\n",
      "|      2017|    5431918|\n",
      "|      2016|    5368391|\n",
      "|      2018|       1057|\n",
      "|      2019|        472|\n",
      "|      2015|        419|\n",
      "|      2000|        185|\n",
      "|      2014|        120|\n",
      "|      2012|         87|\n",
      "|      2013|         70|\n",
      "|      2027|         50|\n",
      "|      2010|         48|\n",
      "|      2026|         24|\n",
      "|      2020|         22|\n",
      "|      2011|         22|\n",
      "|      2021|         22|\n",
      "|      2007|         18|\n",
      "|      2030|         12|\n",
      "|      2006|          8|\n",
      "|      2028|          8|\n",
      "|      2025|          6|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the years in dataset\n",
    "spark.sql('SELECT EXTRACT(YEAR FROM CAST(Issue_Date as DATE)) as issue_year,count(*) as num_records FROM dfTable group by issue_year order by num_records desc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumption:\n",
    "Though there are records for year other than 2017, we are going with the assumption that the whole dataset belongs to year 2017. So total number of tickets is 10803028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find out the number of unique states from where the cars that got parking tickets came.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Registration_State|\n",
      "+------------------+\n",
      "|                AZ|\n",
      "|                SC|\n",
      "|                NS|\n",
      "|                LA|\n",
      "|                MN|\n",
      "|                NJ|\n",
      "|                MX|\n",
      "|                DC|\n",
      "|                OR|\n",
      "|                99|\n",
      "|                NT|\n",
      "|                VA|\n",
      "|                RI|\n",
      "|                WY|\n",
      "|                KY|\n",
      "|                BC|\n",
      "|                NH|\n",
      "|                MI|\n",
      "|                GV|\n",
      "|                NV|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the registration states that got the tickets\n",
    "spark.sql('SELECT distinct Registration_State FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT Registration_State)|\n",
      "+----------------------------------+\n",
      "|                                67|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding the total number of distinct registration states from which the cars got the tickets\n",
    "spark.sql('SELECT COUNT(distinct Registration_State) FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|Registration_State|num_tickets|\n",
      "+------------------+-----------+\n",
      "|                NY|    8481061|\n",
      "|                NJ|     925965|\n",
      "|                PA|     285419|\n",
      "|                FL|     144556|\n",
      "|                CT|     141088|\n",
      "|                MA|      85547|\n",
      "|                IN|      80749|\n",
      "|                VA|      72626|\n",
      "|                MD|      61800|\n",
      "|                NC|      55806|\n",
      "|                IL|      37329|\n",
      "|                GA|      36852|\n",
      "|                99|      36625|\n",
      "|                TX|      36516|\n",
      "|                AZ|      26426|\n",
      "|                OH|      25302|\n",
      "|                CA|      24260|\n",
      "|                SC|      21836|\n",
      "|                ME|      21574|\n",
      "|                MN|      18227|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the number of tickets state-wise in a descending order\n",
    "spark.sql('SELECT Registration_State,COUNT(1) as num_tickets FROM dfTable group by Registration_State order by num_tickets desc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed from the above table that New York(NY) state has the highest number of tickets for faulty car parking of approx 84 Lakhs. But, there is a numeric entry in the registration_state column with state code as 99, which must be erroneous.\n",
    "We will replace it with the state having the maximum entries i.e NY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the state code 99 with NY\n",
    "nyc_taxi = spark.sql(\n",
    "    \"SELECT Summons_Number,\\\n",
    "            Plate_ID,\\\n",
    "            CASE WHEN Registration_State = 99 THEN 'NY' ELSE Registration_State END AS  Registration_State,\\\n",
    "            Issue_Date,\\\n",
    "            Violation_Code,\\\n",
    "            Vehicle_Body_Type,\\\n",
    "            Vehicle_Make,\\\n",
    "            Violation_Precinct,\\\n",
    "            Issuer_Precinct,\\\n",
    "            Violation_Time\\\n",
    "    FROM dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "nyc_taxi.where(col(\"Registration_State\")=='99').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Registration_State|\n",
      "+------------------+\n",
      "|                AZ|\n",
      "|                SC|\n",
      "|                NS|\n",
      "|                LA|\n",
      "|                MN|\n",
      "|                NJ|\n",
      "|                MX|\n",
      "|                DC|\n",
      "|                OR|\n",
      "|                NT|\n",
      "|                VA|\n",
      "|                RI|\n",
      "|                KY|\n",
      "|                WY|\n",
      "|                BC|\n",
      "|                NH|\n",
      "|                MI|\n",
      "|                GV|\n",
      "|                NV|\n",
      "|                QB|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the registration states after handling erroneous data\n",
    "spark.sql('SELECT distinct Registration_State FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT Registration_State)|\n",
      "+----------------------------------+\n",
      "|                                66|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct count of registration states after changing state code 99 to NY\n",
    "spark.sql('SELECT COUNT(distinct Registration_State) FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data manipulation, the count of registration states has decreased by 1 from earlier count of 67 to present count of 66. This is due to the replacement of registartion_state 99 with NY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How often does each violation code occur? Display the frequency of the top five violation codes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation_Code|Frequency_of_Tickets|\n",
      "+--------------+--------------------+\n",
      "|            21|             1528588|\n",
      "|            36|             1400614|\n",
      "|            38|             1062304|\n",
      "|            14|              893498|\n",
      "|            20|              618593|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Violation_Code,count(*) as Frequency_of_Tickets FROM dfTable group by Violation_Code order by Frequency_of_Tickets desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest number of tickets falls under the Violation code 21 which is **Street Cleaning: No parking where parking is not allowed by sign, street marking or traffic control device.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|Vehicle_Body_Type|Frequency_of_Tickets|\n",
      "+-----------------+--------------------+\n",
      "|             SUBN|             3719802|\n",
      "|             4DSD|             3082020|\n",
      "|              VAN|             1411970|\n",
      "|             DELV|              687330|\n",
      "|              SDN|              438191|\n",
      "+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Vehicle_Body_Type,count(*) as Frequency_of_Tickets FROM dfTable group by Vehicle_Body_Type order by Frequency_of_Tickets desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vehicle Body Type Codes have the following interpretation:\n",
    "\n",
    "**[1] SUBN-The law defines a suburban as a vehicle that can be used to carry passengers and cargo. Vehicles that\n",
    "can be registered with the suburban body type include station wagons, sport utility vehicles, hearses and\n",
    "ambulances.**<br>\n",
    "**[2] 4DSD-Four-door sedan**<br>\n",
    "**[3] VAN-Type of road vehicle used for transporting goods or people.**<br>\n",
    "**[4] DELV-Delivery Truck**<br>\n",
    "**[5] SDN-Civilian Sedan**<br>\n",
    "\n",
    "The SUBN body type results in the highest number of tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|Vehicle_Make|Frequency_of_Tickets|\n",
      "+------------+--------------------+\n",
      "|        FORD|             1280958|\n",
      "|       TOYOT|             1211451|\n",
      "|       HONDA|             1079238|\n",
      "|       NISSA|              918590|\n",
      "|       CHEVR|              714655|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Vehicle_Make,count(*) as Frequency_of_Tickets FROM dfTable group by Vehicle_Make order by Frequency_of_Tickets desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ford vehicles contribute to the highest frequency of Parking Tickets, followed by Totyota, Honda, Nissan and Chevrolet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. A precinct is a police station that has a certain zone of the city under its command. Find the (5 highest) frequencies of tickets for each of the following:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Violation Precinct (Precinct of the zone where violation occurred)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|Violation_Precinct|Frequency_of_Tickets|\n",
      "+------------------+--------------------+\n",
      "|                 0|             2072400|\n",
      "|                19|              535671|\n",
      "|                14|              352450|\n",
      "|                 1|              331810|\n",
      "|                18|              306920|\n",
      "|               114|              296514|\n",
      "+------------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Violation_Precinct,count(*) as Frequency_of_Tickets FROM dfTable group by Violation_Precinct order by Frequency_of_Tickets desc').show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precinct named as 0 are erroneous entries. Ignoring that, the Violation Precinct 19 has the highest number of tickets. The top five highest frequency of tickets falls under precincts 19,14,1,18 and 114 respectively.\n",
    "\n",
    "**Violation Code 19 : Bus Stop: Standing or parking where standing is not allowed by sign, street marking or; traffic control device.**<br>\n",
    "**Violation Code 14 : General No Standing: Standing or parking where standing is not allowed by sign, street marking or; traffic control device.**<br>\n",
    "**Violation Code 1 : Failure of an intercity bus to prominently display a copy of an intercity bus permit.**<br>\n",
    "**Violation Code 18 : Bus Lane: Standing or parking where standing is not allowed by sign, street marking or; traffic control device.**<br>\n",
    "\n",
    "It can be inferred that the most parking tickets are issued in places like Bus-stop, Bus Lane and No standing areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Issuer Precinct (Precinct that issued the ticket)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|Issuer_Precinct|Frequency_of_Tickets|\n",
      "+---------------+--------------------+\n",
      "|              0|             2388479|\n",
      "|             19|              521513|\n",
      "|             14|              344977|\n",
      "|              1|              321170|\n",
      "|             18|              296553|\n",
      "|            114|              289950|\n",
      "+---------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Issuer_Precinct,count(*) as Frequency_of_Tickets FROM dfTable group by Issuer_Precinct order by Frequency_of_Tickets desc').show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precinct named as 0 are erroneous entries. Ignoring that, the Issuer Precinct 19 issued the highest number of tickets. The top five highest frequency of tickets are issued by precincts 19,14,1,18 and 114 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Find the violation code frequencies for three precincts that have issued the most number of tickets. Do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 ticket Issuing Precincts Codes ignoring precinct 0 as erroneous, are 19, 14 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|Issuer_Precinct|Violation_Code|Frequency_of_Tickets|\n",
      "+---------------+--------------+--------------------+\n",
      "|             19|            46|               86390|\n",
      "|             19|            37|               72437|\n",
      "|             19|            38|               72344|\n",
      "|             19|            14|               57563|\n",
      "|             19|            21|               54700|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Violation Code Distribution in Issuer Precinct 19\n",
    "spark.sql('SELECT Issuer_Precinct,Violation_Code,count(*) as Frequency_of_Tickets FROM dfTable \\\n",
    "           WHERE Issuer_Precinct = 19\\\n",
    "           group by Issuer_Precinct,Violation_Code order by Issuer_Precinct,Frequency_of_Tickets desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Issuer Precinct 19,the highest number of tickets has been issued for violation code 46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|Issuer_Precinct|Violation_Code|Frequency_of_Tickets|\n",
      "+---------------+--------------+--------------------+\n",
      "|             14|            14|               73837|\n",
      "|             14|            69|               58026|\n",
      "|             14|            31|               39857|\n",
      "|             14|            47|               30540|\n",
      "|             14|            42|               20663|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Violation Code Distribution in Issuer Precinct 14\n",
    "spark.sql('SELECT Issuer_Precinct,Violation_Code,count(*) as Frequency_of_Tickets FROM dfTable \\\n",
    "           WHERE Issuer_Precinct = 14\\\n",
    "           group by Issuer_Precinct,Violation_Code order by Issuer_Precinct,Frequency_of_Tickets desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Issuer Precinct 14,the highest number of tickets has been issued for violation code 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|Issuer_Precinct|Violation_Code|Frequency_of_Tickets|\n",
      "+---------------+--------------+--------------------+\n",
      "|              1|            14|               73522|\n",
      "|              1|            16|               38937|\n",
      "|              1|            20|               27841|\n",
      "|              1|            46|               22534|\n",
      "|              1|            38|               16989|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Violation Code Distribution in Issuer Precinct 1\n",
    "spark.sql('SELECT Issuer_Precinct,Violation_Code,count(*) as Frequency_of_Tickets FROM dfTable \\\n",
    "           WHERE Issuer_Precinct = 1\\\n",
    "           group by Issuer_Precinct,Violation_Code order by Issuer_Precinct,Frequency_of_Tickets desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Issuer Precinct 1,the highest number of tickets has been issued for violation code 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation_Code|Frequency_of_Tickets|\n",
      "+--------------+--------------------+\n",
      "|            21|             1194971|\n",
      "|            38|              966350|\n",
      "|            14|              681488|\n",
      "|            20|              553714|\n",
      "|            37|              507848|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Violation Code Distribution in Other Issuer Precincts\n",
    "spark.sql('SELECT Violation_Code,count(*) as Frequency_of_Tickets FROM dfTable \\\n",
    "           WHERE Issuer_Precinct NOT IN (0,19,14,1)\\\n",
    "           group by Violation_Code order by Frequency_of_Tickets desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other Issuer precinct's, the highest number of tickets has been issued for violation code 21,38 and 14.\n",
    "\n",
    "We aslo observed that among the top three precincts, precinct 14 and 1 has highest number of tickets issued for violation code 14. So violation code 14 which is General No Standing, can be cited as the most common violation code across different precincts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Find out the properties of parking violations across different times of the day:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this activity first we will be doing some data cleansing operations, like dropping null values if any, extract the hour portion from the violation_time column and divide it into different bins for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: long (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Issue_Date: timestamp (nullable = true)\n",
      " |-- Violation_Code: integer (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: integer (nullable = true)\n",
      " |-- Issuer_Precinct: integer (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for any null values in any column\n",
    "spark.sql('SELECT COUNT(1) FROM dfTable \\\n",
    "           WHERE Summons_Number IS NULL or Plate_ID is null or Registration_State is null\\\n",
    "           or Issue_Date is null or Violation_Code is null or Vehicle_Body_Type is null or Vehicle_Make is null\\\n",
    "           or Violation_Precinct is null or Issuer_Precinct is null or Violation_Time is null').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming that only null values will indicate missing values. So as per the result of the above query there are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe by dropping the null values, since there are no null values the count will remain same\n",
    "nyc_taxi_dropnull = nyc_taxi.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_taxi_dropnull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation_Time|\n",
      "+--------------+\n",
      "|         0143A|\n",
      "|         0400P|\n",
      "|         0233P|\n",
      "|         1120A|\n",
      "|         0555P|\n",
      "|         0852P|\n",
      "|         0215A|\n",
      "|         0758A|\n",
      "|         1005A|\n",
      "|         0845A|\n",
      "|         0015A|\n",
      "|         0707A|\n",
      "|         1022A|\n",
      "|         1150A|\n",
      "|         0525A|\n",
      "|         0645P|\n",
      "|         1122A|\n",
      "|         0256P|\n",
      "|         1232A|\n",
      "|         1034A|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the violation time column\n",
    "nyc_taxi_dropnull.select('Violation_Time').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The violation time column has time in the format Hr+Min+TimeMeridiem. For eg: 0143A means 01Hr 43Mins AM. We will extract the hour, mins, and Meridiem part from the violation_time column and create separate columns for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the violation time in Hour Minutes and Meridiem columns\n",
    "from pyspark.sql.functions import substring,concat,lit\n",
    "nyc_taxi_final = nyc_taxi_dropnull.withColumn(\"Violation_Hour\", substring(\"Violation_Time\",1,2))\n",
    "nyc_taxi_final = nyc_taxi_final.withColumn(\"Violation_Minute\", substring(\"Violation_Time\",3,2))\n",
    "nyc_taxi_final = nyc_taxi_final.withColumn(\"Violation_AMPM\", concat(substring(\"Violation_Time\",5,1),lit(\"M\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+----------------+--------------+\n",
      "|Violation_Time|Violation_Hour|Violation_Minute|Violation_AMPM|\n",
      "+--------------+--------------+----------------+--------------+\n",
      "|         0143A|            01|              43|            AM|\n",
      "|         0400P|            04|              00|            PM|\n",
      "|         0233P|            02|              33|            PM|\n",
      "|         1120A|            11|              20|            AM|\n",
      "|         0555P|            05|              55|            PM|\n",
      "|         0852P|            08|              52|            PM|\n",
      "|         0215A|            02|              15|            AM|\n",
      "|         0758A|            07|              58|            AM|\n",
      "|         1005A|            10|              05|            AM|\n",
      "|         0845A|            08|              45|            AM|\n",
      "|         0015A|            00|              15|            AM|\n",
      "|         0707A|            07|              07|            AM|\n",
      "|         1022A|            10|              22|            AM|\n",
      "|         1150A|            11|              50|            AM|\n",
      "|         0525A|            05|              25|            AM|\n",
      "|         0645P|            06|              45|            PM|\n",
      "|         1122A|            11|              22|            AM|\n",
      "|         0256P|            02|              56|            PM|\n",
      "|         1232A|            12|              32|            AM|\n",
      "|         1034A|            10|              34|            AM|\n",
      "+--------------+--------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_taxi_final.select('Violation_Time','Violation_Hour','Violation_Minute','Violation_AMPM').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_final.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation_Time|\n",
      "+--------------+\n",
      "|         4733P|\n",
      "|         2959P|\n",
      "|         6815P|\n",
      "|         1435P|\n",
      "|         1820P|\n",
      "|         8715P|\n",
      "|         4930P|\n",
      "|         5402P|\n",
      "|         4317P|\n",
      "|         5857P|\n",
      "|         6820P|\n",
      "|         3028P|\n",
      "|         1305P|\n",
      "|         5410P|\n",
      "|         7640P|\n",
      "|         2240P|\n",
      "|         8735P|\n",
      "|         6637P|\n",
      "|         5959P|\n",
      "|         8450P|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These are erroneous record having hour > 12\n",
    "spark.sql('SELECT Violation_Time FROM dfTable WHERE Violation_Hour > 12').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation_Hour|\n",
      "+--------------+\n",
      "|            .2|\n",
      "|            .3|\n",
      "|            .9|\n",
      "|            0+|\n",
      "|            0.|\n",
      "|            00|\n",
      "|            01|\n",
      "|            02|\n",
      "|            03|\n",
      "|            04|\n",
      "|            05|\n",
      "|            06|\n",
      "|            07|\n",
      "|            08|\n",
      "|            09|\n",
      "|            10|\n",
      "|            11|\n",
      "|            12|\n",
      "|            13|\n",
      "|            14|\n",
      "|            15|\n",
      "|            16|\n",
      "|            17|\n",
      "|            18|\n",
      "|            20|\n",
      "|            21|\n",
      "|            22|\n",
      "|            23|\n",
      "|            25|\n",
      "|            26|\n",
      "|            27|\n",
      "|            28|\n",
      "|            29|\n",
      "|            30|\n",
      "|            32|\n",
      "|            33|\n",
      "|            34|\n",
      "|            36|\n",
      "|            37|\n",
      "|            38|\n",
      "|            41|\n",
      "|            43|\n",
      "|            46|\n",
      "|            47|\n",
      "|            48|\n",
      "|            49|\n",
      "|            50|\n",
      "|            51|\n",
      "|            52|\n",
      "|            54|\n",
      "|            56|\n",
      "|            57|\n",
      "|            58|\n",
      "|            59|\n",
      "|            60|\n",
      "|            61|\n",
      "|            64|\n",
      "|            65|\n",
      "|            66|\n",
      "|            68|\n",
      "|            70|\n",
      "|            72|\n",
      "|            73|\n",
      "|            76|\n",
      "|            78|\n",
      "|            81|\n",
      "|            82|\n",
      "|            84|\n",
      "|            85|\n",
      "|            86|\n",
      "|            87|\n",
      "|            na|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display all hours present in the dataset\n",
    "spark.sql('SELECT distinct Violation_Hour FROM dfTable order by Violation_Hour').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are lot of erroneous records with Hour as .2, .3, .9, 0+ etc. We will be ignoring them while segregating into bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation_Time|\n",
      "+--------------+\n",
      "|         .240P|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Violation_Time FROM dfTable WHERE Violation_Hour = '.2'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation_Hour|\n",
      "+--------------+\n",
      "|            .3|\n",
      "|            .9|\n",
      "|            0+|\n",
      "|            0.|\n",
      "|            00|\n",
      "|            01|\n",
      "|            02|\n",
      "|            03|\n",
      "|            04|\n",
      "|            05|\n",
      "|            06|\n",
      "|            07|\n",
      "|            08|\n",
      "|            09|\n",
      "|            10|\n",
      "|            11|\n",
      "|            12|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are records that have both 00xxAM as well as 12xxAM. Therefore we will replace all 00xxAM with 12xxAM\n",
    "spark.sql(\"SELECT distinct Violation_Hour FROM dfTable WHERE Violation_AMPM = 'AM' ORDER BY Violation_Hour\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation_Hour|\n",
      "+--------------+\n",
      "|            .2|\n",
      "|            0.|\n",
      "|            00|\n",
      "|            01|\n",
      "|            02|\n",
      "|            03|\n",
      "|            04|\n",
      "|            05|\n",
      "|            06|\n",
      "|            07|\n",
      "|            08|\n",
      "|            09|\n",
      "|            10|\n",
      "|            11|\n",
      "|            12|\n",
      "|            13|\n",
      "|            14|\n",
      "|            15|\n",
      "|            16|\n",
      "|            17|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct Violation_Hour FROM dfTable WHERE Violation_AMPM = 'PM' ORDER BY Violation_Hour\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be converting the 12 hour format into 24-hour format. During this process the erroneous rows will be replaced by null and rows having violation hour as 12 or 0 will be replaced by 0 hour and hour post 12 noon i.e PM will be replaced by hour+12. The data manipulation is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|Violation_Time|24_Violation_Hour|\n",
      "+--------------+-----------------+\n",
      "|         0143A|                1|\n",
      "|         0400P|               16|\n",
      "|         0233P|               14|\n",
      "|         1120A|               11|\n",
      "|         0555P|               17|\n",
      "|         0852P|               20|\n",
      "|         0215A|                2|\n",
      "|         0758A|                7|\n",
      "|         1005A|               10|\n",
      "|         0845A|                8|\n",
      "|         0015A|                0|\n",
      "|         0707A|                7|\n",
      "|         1022A|               10|\n",
      "|         1150A|               11|\n",
      "|         0525A|                5|\n",
      "|         0645P|               18|\n",
      "|         1122A|               11|\n",
      "|         0256P|               14|\n",
      "|         1232A|                0|\n",
      "|         1034A|               10|\n",
      "+--------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Violation_Time,CAST(24_Violation_Hour AS INTEGER) 24_Violation_Hour\\\n",
    "           FROM\\\n",
    "           (\\\n",
    "           SELECT Violation_Time, \\\n",
    "           CASE WHEN Violation_AMPM = 'AM' AND (Violation_Hour = 12 OR Violation_Hour = 0) THEN 0\\\n",
    "                WHEN Violation_AMPM = 'AM' AND Violation_Hour BETWEEN 1 AND 11 THEN Violation_Hour\\\n",
    "                WHEN Violation_AMPM = 'PM' AND (Violation_Hour = 0 OR Violation_Hour = 12) THEN 12\\\n",
    "                WHEN Violation_AMPM = 'PM' AND Violation_Hour BETWEEN 1 AND 11 THEN Violation_Hour+12\\\n",
    "                ELSE NULL END 24_Violation_Hour\\\n",
    "           FROM dfTable)a\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column 24_Violation_Hour to the dataframe having modified hour values \n",
    "nyc_taxi_final = spark.sql(\n",
    "    \"SELECT Summons_Number,\\\n",
    "            Plate_ID,\\\n",
    "            Registration_State,\\\n",
    "            Issue_Date,\\\n",
    "            Violation_Code,\\\n",
    "            Vehicle_Body_Type,\\\n",
    "            Vehicle_Make,\\\n",
    "            Violation_Precinct,\\\n",
    "            Issuer_Precinct,\\\n",
    "            Violation_Time,\\\n",
    "            Violation_Hour,\\\n",
    "            Violation_Minute,\\\n",
    "            Violation_AMPM,\\\n",
    "            CAST(CASE WHEN Violation_AMPM = 'AM' AND (Violation_Hour = 12 OR Violation_Hour = 0) THEN 0\\\n",
    "                WHEN Violation_AMPM = 'AM' AND Violation_Hour BETWEEN 1 AND 11 THEN Violation_Hour\\\n",
    "                WHEN Violation_AMPM = 'PM' AND (Violation_Hour = 0 OR Violation_Hour = 12) THEN 12\\\n",
    "                WHEN Violation_AMPM = 'PM' AND Violation_Hour BETWEEN 1 AND 11 THEN Violation_Hour+12\\\n",
    "                ELSE NULL END AS INTEGER) 24_Violation_Hour\\\n",
    "     FROM dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: long (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Issue_Date: timestamp (nullable = true)\n",
      " |-- Violation_Code: integer (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: integer (nullable = true)\n",
      " |-- Issuer_Precinct: integer (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      " |-- Violation_Hour: string (nullable = true)\n",
      " |-- Violation_Minute: string (nullable = true)\n",
      " |-- Violation_AMPM: string (nullable = true)\n",
      " |-- 24_Violation_Hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_taxi_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_final.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the hour into six equal discrete bins of time.\n",
    "df_violation_hr = spark.sql(\"SELECT 24_Violation_Hour,\\\n",
    "                                       Violation_Code,\\\n",
    "                                       CASE WHEN 24_Violation_Hour BETWEEN 0 AND 3\\\n",
    "                                       THEN '0_3'\\\n",
    "                                       WHEN 24_Violation_Hour BETWEEN 4 AND 7\\\n",
    "                                       THEN '4_7'\\\n",
    "                                       WHEN 24_Violation_Hour BETWEEN 8 AND 11\\\n",
    "                                       THEN '8_11'\\\n",
    "                                       WHEN 24_Violation_Hour BETWEEN 12 AND 15\\\n",
    "                                       THEN '12_15' \\\n",
    "                                       WHEN 24_Violation_Hour BETWEEN 16 AND 19\\\n",
    "                                       THEN '16_19' \\\n",
    "                                       WHEN 24_Violation_Hour BETWEEN 20 AND 23\\\n",
    "                                       THEN '20_23' \\\n",
    "                                       END AS Violation_Hour_Bin\\\n",
    "                                       FROM dfTable WHERE 24_Violation_Hour is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_violation_hr.createOrReplaceTempView(\"df_violation_hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------------+\n",
      "|Violation_Hour_Bin|Violation_Code|Frequency_of_Tickets|\n",
      "+------------------+--------------+--------------------+\n",
      "|             16_19|            38|              203232|\n",
      "|             16_19|            37|              145784|\n",
      "|             16_19|            14|              144749|\n",
      "|              8_11|            21|             1182689|\n",
      "|              8_11|            36|              751422|\n",
      "|              8_11|            38|              346518|\n",
      "|               4_7|            14|              141276|\n",
      "|               4_7|            21|              119469|\n",
      "|               4_7|            40|              112186|\n",
      "|             12_15|            36|              588395|\n",
      "|             12_15|            38|              462758|\n",
      "|             12_15|            37|              337075|\n",
      "|               0_3|            21|               77461|\n",
      "|               0_3|            40|               50948|\n",
      "|               0_3|            78|               32243|\n",
      "|             20_23|             7|               65593|\n",
      "|             20_23|            38|               47029|\n",
      "|             20_23|            14|               44779|\n",
      "+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find three most commonly occurring violations\n",
    "spark.sql(\"SELECT Violation_Hour_Bin,\\\n",
    "                                  Violation_Code,\\\n",
    "                                  Frequency_of_Tickets\\\n",
    "                                  FROM (SELECT Violation_Hour_Bin,\\\n",
    "                                  Violation_Code,\\\n",
    "                                  Frequency_of_Tickets,\\\n",
    "                                  dense_rank() over (partition by Violation_Hour_Bin order by Frequency_of_Tickets desc) Rnk\\\n",
    "                                  FROM (SELECT Violation_Hour_Bin,\\\n",
    "                                  Violation_Code,\\\n",
    "                                  count(*)as Frequency_of_Tickets\\\n",
    "                                  FROM df_violation_hr\\\n",
    "                                  GROUP BY Violation_Hour_Bin,\\\n",
    "                                  Violation_Code))\\\n",
    "                                  WHERE Rnk <= 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the three most commonly occuring violations across different times of the day.\n",
    "\n",
    "While observing the Frequency of Top-3 Violation Codes in each Violation Time Bin it is clear that the majority of the Tickets are issued between 0800-1100 Hrs and 1200-1500 Hrs.<br>\n",
    "\n",
    "It is also important to note the exceptionally high number of tickets for Violation Code 21 [This is expected as Code 21 stands for No-Parking Zone Tickets. Majority of the public might park inappropriately during the morning rush] issued between\n",
    "0800-1100 Hrs. <br>\n",
    "\n",
    "There is also a high frequency of tickets for Violation Code 36 and 38 between 0800-1500 Hrs.\n",
    "[This is also expected as Code 36 is due to exceeding speed limit near school zones, there are spikes between\n",
    "0800-1500 Hrs during start and end of school day while Codes 37&38 are due to parking meter\n",
    "between 1200-1500 Hrs. and 1600-1900 Hrs.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|Violation_Code|no_of_tickets|\n",
      "+--------------+-------------+\n",
      "|            21|      1528546|\n",
      "|            36|      1400614|\n",
      "|            38|      1062301|\n",
      "+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Violation_Code,count(*) as no_of_tickets FROM df_violation_hr \\\n",
    "           group by Violation_Code order by no_of_tickets desc').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes 21, 36 and 38 are most frequent. Now, let's see in which time bins these occur the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-------------+\n",
      "|Violation_Code|Violation_Hour_Bin|no_of_tickets|\n",
      "+--------------+------------------+-------------+\n",
      "|            21|              8_11|      1182689|\n",
      "|            21|             12_15|       148013|\n",
      "|            21|               4_7|       119469|\n",
      "|            21|               0_3|        77461|\n",
      "|            21|             16_19|          551|\n",
      "|            21|             20_23|          363|\n",
      "|            36|              8_11|       751422|\n",
      "|            36|             12_15|       588395|\n",
      "|            36|               4_7|        33939|\n",
      "|            36|             16_19|        26858|\n",
      "|            38|             12_15|       462758|\n",
      "|            38|              8_11|       346518|\n",
      "|            38|             16_19|       203232|\n",
      "|            38|             20_23|        47029|\n",
      "|            38|               4_7|         2300|\n",
      "|            38|               0_3|          464|\n",
      "+--------------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Violation_Code,\\\n",
    "          Violation_Hour_Bin,\\\n",
    "          count(*) no_of_tickets\\\n",
    "          FROM df_violation_hr\\\n",
    "          WHERE violation_code IN (21,36,38)\\\n",
    "          GROUP BY Violation_Code,Violation_Hour_Bin\\\n",
    "          ORDER BY Violation_Code,no_of_tickets desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The violation code 21 and 36 issue maximum tickets in the morning time 0800-1100 Hrs. The code 38 is more frequently violated in the span of 1200-1500 Hrs, followed by next maximum tickets in the  0800-1100 Hrs time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Find some seasonality in this data: Divide the year into a certain number of seasons and find the frequencies of tickets for each season. Then, find the three most common violations for each of these seasons.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: long (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Issue_Date: timestamp (nullable = true)\n",
      " |-- Violation_Code: integer (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: integer (nullable = true)\n",
      " |-- Issuer_Precinct: integer (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      " |-- Violation_Hour: string (nullable = true)\n",
      " |-- Violation_Minute: string (nullable = true)\n",
      " |-- Violation_AMPM: string (nullable = true)\n",
      " |-- 24_Violation_Hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_taxi_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_final.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addingcolumn issue_mon as the month in which the ticket has been issued\n",
    "nyc_taxi_final=spark.sql(\"SELECT Summons_Number,\\\n",
    "            Plate_ID,\\\n",
    "            Registration_State,\\\n",
    "            Issue_Date,\\\n",
    "            Violation_Code,\\\n",
    "            Vehicle_Body_Type,\\\n",
    "            Vehicle_Make,\\\n",
    "            Violation_Precinct,\\\n",
    "            Issuer_Precinct,\\\n",
    "            Violation_Time,\\\n",
    "            Violation_Hour,\\\n",
    "            Violation_Minute,\\\n",
    "            Violation_AMPM,\\\n",
    "            24_Violation_Hour,\\\n",
    "            CAST(EXTRACT(MONTH FROM CAST(Issue_Date as DATE)) AS String) issue_mon\\\n",
    "            FROM dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: long (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Issue_Date: timestamp (nullable = true)\n",
      " |-- Violation_Code: integer (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: integer (nullable = true)\n",
      " |-- Issuer_Precinct: integer (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      " |-- Violation_Hour: string (nullable = true)\n",
      " |-- Violation_Minute: string (nullable = true)\n",
      " |-- Violation_AMPM: string (nullable = true)\n",
      " |-- 24_Violation_Hour: integer (nullable = true)\n",
      " |-- issue_mon: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_taxi_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_final.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nyc_season=spark.sql(\"select issue_mon,Violation_Code,\\\n",
    "            CASE WHEN issue_mon IN (1,2,12)\\\n",
    "            THEN 'WINTER'\\\n",
    "            WHEN issue_mon BETWEEN 9 AND 11\\\n",
    "            THEN 'FALL'\\\n",
    "            WHEN issue_mon BETWEEN 3 AND 5\\\n",
    "            THEN 'SPRING'\\\n",
    "            WHEN issue_mon BETWEEN 6 AND 8\\\n",
    "            THEN 'SUMMER'\\\n",
    "            WHEN issue_mon like 12 THEN 'WINTER'\\\n",
    "             else  NULL\\\n",
    "            END AS SEASON\\\n",
    "            from dfTable where issue_mon is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+\n",
      "|issue_mon|Violation_Code|SEASON|\n",
      "+---------+--------------+------+\n",
      "|        7|             7|SUMMER|\n",
      "|        7|             7|SUMMER|\n",
      "|        8|             5|SUMMER|\n",
      "|        6|            47|SUMMER|\n",
      "|       11|            69|  FALL|\n",
      "+---------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_season.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_season.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|SEASON|Frequency_of_Tickets|\n",
      "+------+--------------------+\n",
      "|SPRING|             2880687|\n",
      "|  FALL|             2830802|\n",
      "|SUMMER|             2606208|\n",
      "|WINTER|             2485331|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the frequencies of tickets for each season\n",
    "spark.sql('select SEASON,count(*) as Frequency_of_Tickets from dfTable\\\n",
    "            group by SEASON\\\n",
    "            order by Frequency_of_Tickets desc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the seasons Spring and Fall account for the highest number of parking tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+\n",
      "|Season|Violation_Code|Frequency_of_Tickets|\n",
      "+------+--------------+--------------------+\n",
      "|WINTER|            21|              362341|\n",
      "|WINTER|            36|              359338|\n",
      "|WINTER|            38|              259723|\n",
      "|SPRING|            21|              402807|\n",
      "|SPRING|            36|              344834|\n",
      "|SPRING|            38|              271192|\n",
      "|  FALL|            36|              456046|\n",
      "|  FALL|            21|              357479|\n",
      "|  FALL|            38|              283828|\n",
      "|SUMMER|            21|              405961|\n",
      "|SUMMER|            38|              247561|\n",
      "|SUMMER|            36|              240396|\n",
      "+------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Season,\\\n",
    "                                  Violation_Code,\\\n",
    "                                  Frequency_of_Tickets\\\n",
    "                                  FROM (SELECT Season,\\\n",
    "                                  Violation_Code,\\\n",
    "                                  Frequency_of_Tickets,\\\n",
    "                                  dense_rank() over (partition by Season order by Frequency_of_Tickets desc) Rnk\\\n",
    "                                  FROM (SELECT Season,\\\n",
    "                                  Violation_Code,\\\n",
    "                                  count(*)as Frequency_of_Tickets\\\n",
    "                                  FROM dfTable\\\n",
    "                                  GROUP BY Season,\\\n",
    "                                  Violation_Code))\\\n",
    "                                  WHERE Rnk <= 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except Fall season, it can be seen that the highest number of tickets are issued for violation code 21. For Fall season, highest number of tickets are issued for violation code 36."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Estimate this for the three most commonly occurring codes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_final.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total occurrences of the three most common violation codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation_Code|Frequency_of_Tickets|\n",
      "+--------------+--------------------+\n",
      "|            21|             1528588|\n",
      "|            36|             1400614|\n",
      "|            38|             1062304|\n",
      "+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_violation = spark.sql('select Violation_Code, count(*) as Frequency_of_Tickets from dfTable group by Violation_Code order by Frequency_of_Tickets desc')\n",
    "top_violation.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Total_tickets_for_top_3_violations|\n",
      "+----------------------------------+\n",
      "|                           3991506|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "top_violation.where('Violation_Code in (21,36,38)').select(sum('Frequency_of_Tickets').alias(\"Total_tickets_for_top_3_violations\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top three most common violation codes are 21,36 and 38.\n",
    "**[21: No Parking Violation, 36: Exceeding speed limit near School Zone, 38: Failed to show receipt or tag in windshield].**\n",
    "The highest frequency of tickets was issued for Violation Code 21 followed by 36 and 38 respectively.\n",
    "Total 3991506 tickets has been issued for the top 3 violation codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The fines associated with different violation codes are divided into two categories: one for the highest-density locations in the city and the other for the rest of the city.**\n",
    "\n",
    "**For the sake of simplicity, take the average of the two. Find the total amount collected for the three violation codes with the maximum tickets. State the code that has the highest total collection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection=spark.sql(\"select Violation_Code, count(*) as freq_tickets,\\\n",
    "          CASE WHEN Violation_Code like 21 THEN CAST((65+45)/2 as INTEGER)\\\n",
    "          WHEN Violation_Code like 36 THEN CAST((50+50)/2 as INTEGER)\\\n",
    "          WHEN Violation_Code like 38 THEN CAST((65+35)/2 as INTEGER)\\\n",
    "          END AS FINE\\\n",
    "        from dfTable where Violation_Code IN (21,36,38) \\\n",
    "        group by Violation_Code order by freq_tickets desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+----+\n",
      "|Violation_Code|freq_tickets|FINE|\n",
      "+--------------+------------+----+\n",
      "|            21|     1528588|  55|\n",
      "|            36|     1400614|  50|\n",
      "|            38|     1062304|  50|\n",
      "+--------------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collection.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+----+----------------+\n",
      "|Violation_Code|freq_tickets|FINE|total_collection|\n",
      "+--------------+------------+----+----------------+\n",
      "|            21|     1528588|  55|        84072340|\n",
      "|            36|     1400614|  50|        70030700|\n",
      "|            38|     1062304|  50|        53115200|\n",
      "+--------------+------------+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total amount collected for the three violation codes with the maximum tickets\n",
    "spark.sql(\"select Violation_Code,freq_tickets,FINE,freq_tickets*FINE as total_collection from dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum revenue comes from the violation against code 21 which prohibits parking in areas having no parking sign, followed by code 36 which is violated if driven above the speedlimit near a school and 38 which is imposed on failure to show a receipt or tag in the windshield.\n",
    "\n",
    "**Violation Code 21 brought in \\$84 million in total fine, while Code 36 and 38 brought in \\$70 million and \\$53 million respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Inference :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business inferences drawn from the above project are as follows:\n",
    "    \n",
    "- The car type SUBN issues the maximum number of tickets around 0.87 million.\n",
    "- The car types of Ford and Toyota issue maximum tickets around 12 Lakhs, when compared to other company models.\n",
    "- The issuer precincts 19 issue maximum number of tickets against violation code 46 which restricts double parking and is mostly common in Mid Manhattan. Due to high demand of service delivery, this might occur and must be taken into checklist. Where as, precincts 14 and 1 issue maximum tickets against code 14 which is violated due to General No Standing: Standing or parking where standing is not allowed by sign, street marking or; traffic control device.\n",
    "- The morning slot of 8-11 AM issue maximum number of tickets against code 21 (illegal parking) and 36 (driving beyond speed limit near a school). This might happen due to rush hours in the office-school time. \n",
    "- If we look into the data season-wise, then also code 21 is violated the most, followed by 36, thereby resulting in a huge revenue in these two.\n",
    "\n",
    "**Hence from the EDA on the NYC Parking data, it is clear that from all the views created the most problematic area in the parking field is parking in the wrong place and breaking the speed limit. The department must dig in more in order to find out the driving factors leading to this and attempting to solve this. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
